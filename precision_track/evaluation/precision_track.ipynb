{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bf5f38",
   "metadata": {},
   "source": [
    "# Environment Setup for PrecisionTrack Evaluation\n",
    "\n",
    "## Introduction\n",
    "This notebook is dedicated to evaluating the inference performance of PrecisionTrack in terms of both tracking quality and latency. Below are the steps to set up the necessary environment on Ubuntu 22.04.\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Create Conda Environment\n",
    "To begin, create a Conda environment named `precision_track` with the necessary packages. Run the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "conda create -n precision_track python==3.11.13\n",
    "conda activate precision_track\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "pip install precision_track\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a00a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "from mmengine import Config\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "\n",
    "from precision_track import PipelinedTracker, Result, Visualizer\n",
    "from precision_track.utils import VideoReader, infer_paths, find_path_in_dir\n",
    "from precision_track.evaluation.utils.mot import evaluate_mot\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e994722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"../configs/tasks/tracking.py\"\n",
    "video_path = \"../assets/20mice.avi\"\n",
    "sink = \"../assets/20mice_w_results.mp4\"\n",
    "gt_path = \"../tests/work_dir/gt_20mice.csv\"\n",
    "cfg = Config.fromfile(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e896d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/12 12:10:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Inference backend set to: TensorRT: 10.11.0.33 with FP16 precision on device: cuda.\n",
      "06/12 12:10:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Dynamically updating tracking thresholds to: {'low_thr': 0.05, 'conf_thr': 0.35, 'init_thr': 0.65}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1471it [00:18, 80.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output: /home/vincent/Documents/precision_track/work_dir/bboxes.csv\n",
      "Saved output: /home/vincent/Documents/precision_track/work_dir/kpts.csv\n",
      "Saved output: /home/vincent/Documents/precision_track/work_dir/velocities.csv\n",
      "Saved output: /home/vincent/Documents/precision_track/work_dir/search_areas.csv\n",
      "Inference + saving the results took about 0.014s per frame. Giving us a total performance of: 71.525 FPS.\n"
     ]
    }
   ],
   "source": [
    "def launch_tracking(config, video):\n",
    "    video = VideoReader(video)\n",
    "    nb_cpu_cores = psutil.cpu_count(logical=False)\n",
    "    if nb_cpu_cores >= 3 and config.pipelined:\n",
    "        tracker = PipelinedTracker(\n",
    "            detector=config.get(\"detector\"),\n",
    "            assigner=config.get(\"assigner\"),\n",
    "            validator=config.get(\"validator\"),  # With Tailtags\n",
    "            analyzer=None,\n",
    "            outputs=config.get(\"outputs\"),\n",
    "            expected_resolution=video.resolution + (3,),\n",
    "            batch_size=config.get(\"batch_size\"),\n",
    "            verbose=True,\n",
    "        )\n",
    "        t0 = time.perf_counter()\n",
    "        tracker(video=video)\n",
    "    delay_per_frame = (time.perf_counter() - t0) / len(video)\n",
    "    print(f\"Inference + saving the results took about {delay_per_frame:.3f}s per frame. Giving us a total performance of: {(1/delay_per_frame):.3f} FPS.\")\n",
    "\n",
    "\n",
    "launch_tracking(config=cfg, video=video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8202eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1 labelled videos...\n",
      "Progress: |██████████████████████████████████████████████████| 1470/1470\n",
      "\n",
      "| Metric                  |     Score |\n",
      "|-------------------------|-----------|\n",
      "| MOTA on mouse           |     0.956 |\n",
      "| IDF1 on mouse           |     0.943 |\n",
      "| IDP on mouse            |     0.951 |\n",
      "| IDR on mouse            |     0.936 |\n",
      "| Precision on mouse      |     0.986 |\n",
      "| Recall on mouse         |     0.970 |\n",
      "| IDFP on mouse           |  1414.000 |\n",
      "| IDFN on mouse           |  1881.000 |\n",
      "| IDTP on mouse           | 27335.000 |\n",
      "| Num Switches on mouse   |     8.000 |\n",
      "| Num Detections on mouse | 28341.000 |\n",
      "Weighted averages: MOTA=0.956, IDF1=0.943.\n"
     ]
    }
   ],
   "source": [
    "metafile = cfg.get(\"metainfo\", None)\n",
    "gt_paths = infer_paths(gt_path)\n",
    "result_paths = [cfg[\"outputs\"][0][\"path\"]]\n",
    "\n",
    "assert len(gt_paths) == len(result_paths)\n",
    "gt_r_map = []\n",
    "for i, gt in enumerate(gt_paths):\n",
    "    assert os.path.exists(gt)\n",
    "    gt_name = os.path.splitext(os.path.basename(gt))[0].replace(\"gt_\", \"\")\n",
    "    gt_idx = find_path_in_dir(gt_name, result_paths)\n",
    "    gt_r_map.append((i, gt_idx))\n",
    "\n",
    "print(f\"Evaluating on {len(result_paths)} labelled videos...\")\n",
    "motas = []\n",
    "idf1s = []\n",
    "weigths = []\n",
    "for i in gt_r_map:\n",
    "    gt_idx, result_idx = i\n",
    "\n",
    "    evaluation_results = evaluate_mot(\n",
    "        result_paths[result_idx],\n",
    "        gt_paths[gt_idx],\n",
    "        metafile,\n",
    "        save_path=None,\n",
    "        verbose=True,\n",
    "    )\n",
    "    for cls_metrics in evaluation_results.values():\n",
    "        motas.append(cls_metrics[\"mota\"])\n",
    "        idf1s.append(cls_metrics[\"idf1\"])\n",
    "        weigths.append(cls_metrics[\"num_detections\"])\n",
    "total_detections = sum(weigths)\n",
    "weigths = [w / total_detections for w in weigths]\n",
    "avg_mota = sum(m * w for m, w in zip(motas, weigths))\n",
    "avg_idf1 = sum(i * w for i, w in zip(idf1s, weigths))\n",
    "print(f\"Weighted averages: MOTA={avg_mota:.3f}, IDF1={avg_idf1:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98ca6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1471/1471, 19.0 task/s, elapsed: 77s, ETA:     0s\n",
      "A qualitative assessment of the tracking performances has been saved at ../assets/20mice_w_results.mp4.\n"
     ]
    }
   ],
   "source": [
    "result = Result(outputs=cfg.get(\"outputs\"))\n",
    "result.read()\n",
    "visualizer = Visualizer(**cfg.get(\"visualizer\"))\n",
    "visualizer(video_path, result, sink)\n",
    "print(f\"A qualitative assessment of the tracking performances has been saved at {sink}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
