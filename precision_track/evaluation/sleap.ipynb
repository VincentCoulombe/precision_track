{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Environment Setup for SLEAP Evaluation\n",
    "\n",
    "## Introduction\n",
    "This notebook is dedicated to evaluating the inference performance of SLEAP in terms of both tracking quality and latency. Below are the steps to set up the necessary environment on Ubuntu 22.04.\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Create Conda Environment\n",
    "To begin, create a Conda environment named `sleap` with the necessary packages. Run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "mamba create -y -n sleap -c conda-forge -c nvidia -c sleap -c anaconda sleap=1.3.3\n",
    "```\n",
    "\n",
    "### Additional Libraries\n",
    "After setting up the Conda environment, install the following Python libraries using pip:\n",
    "\n",
    "- **tqdm** for progress bars:\n",
    "  ```bash\n",
    "  pip install tqdm==4.66.4\n",
    "  ```\n",
    "- **tabulate** for table creation and formatting:\n",
    "  ```bash\n",
    "  pip install tabulate==0.9.0\n",
    "  ```\n",
    "- **motmetrics** for computing metrics for object tracking:\n",
    "  ```bash\n",
    "  pip install motmetrics==1.4.0\n",
    "  ```\n",
    "- **OpenCV** for image processing tasks:\n",
    "  ```bash\n",
    "  pip install opencv-python==4.1.2.30\n",
    "  ```\n",
    "- **mmengine** for the registry :\n",
    "  ```bash\n",
    "  pip install mmengine-lite\n",
    "  ```\n",
    "## Next Steps\n",
    "Proceed to the next section once the environment setup is complete to start the evaluation process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sleap\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "from time import perf_counter\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import abc\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from typing import Any, List\n",
    "from tabulate import tabulate\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from mmengine.config import Config\n",
    "\n",
    "from utils.sleap133_inference_w_timer import load_model\n",
    "from utils.downsize_video import downsize_video\n",
    "from methods.mot import MOTEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_latency(times: np.ndarray, title, buffer_size=5, precision=3):\n",
    "    assert len(times) >= buffer_size\n",
    "    times = times[buffer_size:]\n",
    "    mean = np.mean(times)\n",
    "    table = [\n",
    "        [\"Mean\", mean],\n",
    "        [\"Std\", np.std(times)],\n",
    "        [\"Min\", np.min(times)],\n",
    "        [\"Median\", np.median(times)],\n",
    "        [\"Max\", np.max(times)],\n",
    "    ]\n",
    "    print(\n",
    "        \"\\n\"\n",
    "        + tabulate(\n",
    "            table,\n",
    "            headers=[title, \"Inference Time (s)\"],\n",
    "            tablefmt=\"github\",\n",
    "            floatfmt=f\".{precision}f\",\n",
    "            stralign=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return mean\n",
    "\n",
    "\n",
    "def display_mot_results(evaluation: dict, precision=3):\n",
    "    table = []\n",
    "    for cls, metrics in evaluation.items():\n",
    "        table.append([f\"MOTA on {cls}\", metrics[\"mota\"]])\n",
    "        table.append([f\"IDF1 on {cls}\", metrics[\"idf1\"]])\n",
    "        table.append([f\"IDP on {cls}\", metrics[\"idp\"]])\n",
    "        table.append([f\"IDR on {cls}\", metrics[\"idr\"]])\n",
    "        table.append([f\"Precision on {cls}\", metrics[\"precision\"]])\n",
    "        table.append([f\"Recall on {cls}\", metrics[\"recall\"]])\n",
    "        table.append([f\"IDFP on {cls}\", int(metrics[\"idfp\"])])\n",
    "        table.append([f\"IDFN on {cls}\", int(metrics[\"idfn\"])])\n",
    "        table.append([f\"IDTP on {cls}\", int(metrics[\"idtp\"])])\n",
    "        table.append([f\"Num Switches on {cls}\", int(metrics[\"num_switches\"])])\n",
    "        table.append([f\"Num Detections on {cls}\", int(metrics[\"num_detections\"])])\n",
    "    print(\n",
    "        \"\\n\"\n",
    "        + tabulate(\n",
    "            table,\n",
    "            headers=[\"Metric\", \"Score\"],\n",
    "            tablefmt=\"github\",\n",
    "            floatfmt=f\".{precision}f\",\n",
    "            stralign=\"left\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_cxcywh(keypoints: np.ndarray) -> np.ndarray:\n",
    "    mask = ~np.isnan(keypoints).any(1)\n",
    "    if not mask.any():\n",
    "        return np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "    keypoints = keypoints[mask]\n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    xmin, xmax = np.min(x), np.max(x)\n",
    "    ymin, ymax = np.min(y), np.max(y)\n",
    "    w, h = xmax - xmin, ymax - ymin\n",
    "    cx, cy = (xmin + xmax) / 2, (ymin + ymax) / 2\n",
    "    return np.array([cx, cy, w, h], dtype=np.float32)\n",
    "\n",
    "\n",
    "def cxcywh_xywh(cxcywh: np.ndarray) -> np.ndarray:\n",
    "    if np.isnan(cxcywh).any():\n",
    "        return np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "    cx, cy, w, h = cxcywh\n",
    "    return np.array([cx - w / 2, cy - h / 2, w, h], dtype=np.float32)\n",
    "\n",
    "\n",
    "def to_numpy(x):\n",
    "    if isinstance(x, list):\n",
    "        return np.array(x)\n",
    "    elif isinstance(x, (np.ndarray, int, float, str, np.generic)):\n",
    "        return x\n",
    "    else:\n",
    "        raise TypeError(f\"{type(x)} not yet supported.\")\n",
    "\n",
    "\n",
    "class BaseCsvOutput():\n",
    "    SUPPORTED_PRECISION = {32: \"float32\", 64: \"float64\"}\n",
    "    EXTENSION = \".csv\"\n",
    "    MAPPING_EXTENSION = \".npy\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        instance_data: str,\n",
    "        columns: List[str],\n",
    "        confidence_threshold: float = 0.5,\n",
    "        precision: int = 32,\n",
    "    ) -> None:\n",
    "        \"\"\"A BaseCsvOutput iteratively store the relevant dict's instances data\n",
    "        and can save it to csv afterward.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path for either save the content of a BaseCsvOutput to a csv or to read a csv into a BaseCsvOutput\n",
    "            instance_data (str): The name of the relevant instances data inside the dict\n",
    "            columns (List[str]): The name of the csv's columns\n",
    "            confidence_threshold (float, optional): The threshold from which data is retained. Defaults to 0.5.\n",
    "            precision (int, optional): The saving precision for the data. Defaults to 32.\n",
    "        \"\"\"\n",
    "        self.frame_id_mapping = OrderedDict()\n",
    "        self.results = []\n",
    "        self.curr_frame_idx = 0\n",
    "        if precision not in self.SUPPORTED_PRECISION:\n",
    "            raise ValueError(f\"Precision {precision} not supported. Supported precisions are {list(self.SUPPORTED_PRECISION.keys())}\")\n",
    "        self.precision = precision\n",
    "        self.columns = columns\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.supported_instance_data = [\"pred_track_instances\", \"pred_instances\"]\n",
    "        self.instance_data = instance_data\n",
    "        self._setup_path(path)\n",
    "\n",
    "    def _setup_path(self, path: str):\n",
    "        raw_path, _ = os.path.splitext(path)\n",
    "        path = f\"{raw_path}{self.EXTENSION }\"\n",
    "        self.path = os.path.abspath(path)\n",
    "        self.mapping_path = os.path.abspath(f\"{raw_path}_mapping{self.MAPPING_EXTENSION }\")\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, data: Any) -> None:\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return next(reversed(self.frame_id_mapping))\n",
    "\n",
    "    def __getitem__(self, frame_id: int) -> List[Any]:\n",
    "        idx_range = self.frame_id_mapping.get(frame_id, None)\n",
    "        if idx_range is None:\n",
    "            return\n",
    "        return self.results[idx_range[0] : idx_range[1]]\n",
    "\n",
    "    def save(self) -> None:\n",
    "        \"\"\"Save the data to csv and also a mapping of the data (for faster\n",
    "        __getitem__).\n",
    "\n",
    "        Saves only one frame_id_mapping\n",
    "        \"\"\"\n",
    "        formatted_results = pd.DataFrame(self.results, columns=[\"frame_id\", \"class_id\", \"instance_id\"] + self.columns)\n",
    "        formatted_results[\"frame_id\"] = formatted_results[\"frame_id\"].astype(\"uint32\")\n",
    "        formatted_results[\"class_id\"] = formatted_results[\"class_id\"].astype(\"uint16\")\n",
    "        formatted_results[\"instance_id\"] = formatted_results[\"instance_id\"].astype(\"int16\")\n",
    "        for col in self.columns:\n",
    "            formatted_results[col] = formatted_results[col].astype(self.SUPPORTED_PRECISION[self.precision])\n",
    "        formatted_results.to_csv(self.path, index=False)\n",
    "        np.save(self.mapping_path, self.frame_id_mapping, allow_pickle=True)\n",
    "\n",
    "    def read(self) -> None:\n",
    "        \"\"\"Load a csv and a mapping (for faster __getitem__)\"\"\"\n",
    "        assert os.path.exists(self.path), f\"{self.path} does not exist.\"\n",
    "        assert os.path.exists(self.mapping_path), (\n",
    "            f\"To ensure fast iteration through the {os.path.basename(self.path)} file,\"\n",
    "            \"you should keep its provided mapping. Which is expected to be at\"\n",
    "            f\"{self.mapping_path}, but does not exist.\"\n",
    "        )\n",
    "        self.results = pd.read_csv(self.path).values.tolist()\n",
    "        self.frame_id_mapping = np.load(self.mapping_path, allow_pickle=True).item()\n",
    "        self.curr_frame_idx = self.frame_id_mapping[self.__len__()][1] + 1\n",
    "\n",
    "    def _add_row(self, *args) -> None:\n",
    "        self.results.append(list(args))\n",
    "\n",
    "    def _update_frame_id_mapping(self, frame_id: int, increment: int):\n",
    "        if frame_id not in self.frame_id_mapping:\n",
    "            curr_frame_idx = self.curr_frame_idx + increment\n",
    "            self.frame_id_mapping[frame_id] = (\n",
    "                self.curr_frame_idx,\n",
    "                curr_frame_idx,\n",
    "            )\n",
    "            self.curr_frame_idx = curr_frame_idx\n",
    "\n",
    "    def _set_ids(self, instance_data: dict):\n",
    "        return (\n",
    "            np.zeros_like(instance_data[\"labels\"]) - 1\n",
    "            if self.instance_data\n",
    "            not in [\n",
    "                \"pred_track_instances\",\n",
    "                \"validation_instances\",\n",
    "                \"correction_instances\",\n",
    "            ]\n",
    "            else instance_data[\"instances_id\"]\n",
    "        )\n",
    "\n",
    "    def _get_ds_info(self, data_sample: dict):\n",
    "        instance_data = data_sample.get(self.instance_data, None)\n",
    "        if instance_data is None:\n",
    "            raise ValueError(f\"The provided data sample do not contain the expected instance data ({self.instance_data}).\")\n",
    "        return instance_data, data_sample[\"img_id\"]\n",
    "\n",
    "\n",
    "class CsvBoundingBoxes(BaseCsvOutput):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        bbox_format: str = \"xywh\",\n",
    "        instance_data: str = \"pred_instances\",\n",
    "        confidence_threshold: float = 0.5,\n",
    "        precision: int = 32,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            path=path,\n",
    "            precision=precision,\n",
    "            confidence_threshold=confidence_threshold,\n",
    "            columns=[\"x\", \"y\", \"w\", \"h\", \"scores\"],\n",
    "            instance_data=instance_data,\n",
    "        )\n",
    "        self.bbox_format = bbox_format\n",
    "        assert self.instance_data in self.supported_instance_data, f\"The provided instance_data must be one one {self.supported_instance_data}\"\n",
    "\n",
    "    def __call__(self, det_data_sample: dict):\n",
    "        instance_data, frame_id = self._get_ds_info(det_data_sample)\n",
    "        ids = self._set_ids(instance_data)\n",
    "        i = 0\n",
    "        for id_, label, bbox, score in zip(\n",
    "            ids,\n",
    "            instance_data[\"labels\"],\n",
    "            instance_data[\"bboxes\"],\n",
    "            instance_data[\"scores\"],\n",
    "        ):\n",
    "            if (score >= self.confidence_threshold and self.instance_data == \"pred_instances\") or (id_ >= 0 and self.instance_data == \"pred_track_instances\"):\n",
    "                self._add_row(frame_id, label, id_, *bbox, score)\n",
    "                i += 1\n",
    "        self._update_frame_id_mapping(frame_id, i)\n",
    "\n",
    "\n",
    "class CsvKeypoints(BaseCsvOutput):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        instance_data: str = \"pred_instances\",\n",
    "        confidence_threshold: float = 0.5,\n",
    "        precision: int = 32,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            path,\n",
    "            precision=precision,\n",
    "            confidence_threshold=confidence_threshold,\n",
    "            columns=[],\n",
    "            instance_data=instance_data,\n",
    "        )\n",
    "        assert self.instance_data in self.supported_instance_data, f\"The provided instance_data must be one one {self.supported_instance_data}\"\n",
    "\n",
    "    def __call__(self, det_data_sample):\n",
    "        instance_data, frame_id = self._get_ds_info(det_data_sample)\n",
    "        ids = self._set_ids(instance_data)\n",
    "        i = 0\n",
    "        for id_, label, keypoints, scores, score in zip(\n",
    "            ids,\n",
    "            instance_data[\"labels\"],\n",
    "            instance_data[\"keypoints\"],\n",
    "            instance_data[\"keypoint_scores\"],\n",
    "            instance_data[\"scores\"],\n",
    "        ):\n",
    "            label = to_numpy(label)\n",
    "            keypoints = to_numpy(keypoints)\n",
    "            keypoint_scores = to_numpy(scores)\n",
    "            score = to_numpy(score)\n",
    "            if (score >= self.confidence_threshold and self.instance_data in [\"pred_instances\", \"gt_instances\"]) or (\n",
    "                id_ >= 0 and self.instance_data == \"pred_track_instances\"\n",
    "            ):\n",
    "                poses = np.concatenate((keypoints.reshape(1, -1, 2), keypoint_scores.reshape(1, -1, 1)), axis=2)\n",
    "                poses = np.nan_to_num(poses, nan=0.0).flatten().tolist()\n",
    "                self._add_row(frame_id, label, id_, poses)\n",
    "                i += 1\n",
    "        self._update_frame_id_mapping(frame_id, i)\n",
    "\n",
    "    def _add_row(self, frame_id, class_id, object_id, keypoints):\n",
    "        self._set_columns(frame_id, keypoints)\n",
    "        super()._add_row(frame_id, class_id, object_id, *keypoints)\n",
    "\n",
    "    def _set_columns(self, frame_id: int, keypoints: list):\n",
    "        if not self.columns:\n",
    "            self.columns = [f\"{coord}{i}\" for i in range(len(keypoints) // 2) for coord in (\"x\", \"y\")]\n",
    "        else:\n",
    "            assert len(keypoints) == len(self.columns), f\"Inconsistent number of keypoints: {len(keypoints)}, expected: {len(self.columns)} as frame{frame_id}\"\n",
    "\n",
    "\n",
    "class SLEAPOutput:\n",
    "    def __init__(self, save_path_bboxes: str, save_path_kpts: str, precision: int = 32):\n",
    "        self.outputs = [\n",
    "            CsvBoundingBoxes(save_path_bboxes, precision=precision, instance_data=\"pred_track_instances\"),\n",
    "            CsvKeypoints(save_path_kpts, precision=precision, instance_data=\"pred_track_instances\"),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, labeled_frame, x_scale=1, y_scale=1):\n",
    "        frame_idx = labeled_frame.frame_idx\n",
    "        data_sample = {\n",
    "            \"pred_track_instances\": {\n",
    "                \"labels\": [],\n",
    "                \"bboxes\": [],\n",
    "                \"instances_id\": [],\n",
    "                \"keypoints\": [],\n",
    "                \"keypoint_scores\": [],\n",
    "                \"scores\": [],\n",
    "            },\n",
    "            \"img_id\": frame_idx,\n",
    "        }\n",
    "        for inst in labeled_frame.instances:\n",
    "            inst_id = int(re.search(r\"(\\d+)\", inst.track.name).group(0))\n",
    "            inst_kpts = inst.get_points_array(invisible_as_nan=False)\n",
    "            inst_kpts[:, 0] = inst_kpts[:, 0] * x_scale\n",
    "            inst_kpts[:, 1] = inst_kpts[:, 1] * y_scale\n",
    "            inst_bbox = keypoints_cxcywh(inst_kpts)\n",
    "            inst_bbox = cxcywh_xywh(inst_bbox)\n",
    "            class_id = 0\n",
    "            if not np.all(np.isnan(inst_bbox)) and not np.any(inst_bbox ==0):\n",
    "                data_sample[\"pred_track_instances\"][\"labels\"].append(class_id)\n",
    "                data_sample[\"pred_track_instances\"][\"instances_id\"].append(inst_id)\n",
    "                data_sample[\"pred_track_instances\"][\"bboxes\"].append(inst_bbox)\n",
    "                data_sample[\"pred_track_instances\"][\"scores\"].append(1)\n",
    "                inst_kpts = np.nan_to_num(np.concatenate(inst_kpts), nan=0.0).astype(int)\n",
    "                data_sample[\"pred_track_instances\"][\"keypoints\"].append(inst_kpts.tolist())\n",
    "                data_sample[\"pred_track_instances\"][\"keypoint_scores\"].append(np.ones(inst_kpts.shape[0]//2).tolist())\n",
    "\n",
    "        for o in self.outputs:\n",
    "            o(data_sample)\n",
    "\n",
    "    def save(self):\n",
    "        for o in self.outputs:\n",
    "            o.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the SLEAP config\n",
    "\n",
    "You need to make sure you have a valid SLEAP checkpoint trained on MICE. For more informations, please visit SLEAP's documentation: https://sleap.ai/tutorials/initial-training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"../../configs/sleap/evaluation.py\"\n",
    "cfg = Config.fromfile(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Comparison Standards\n",
    "\n",
    "Since SLEAP do not inherently adjust the image resolution during tracking. The only way to ensure a fair comparison is to resize the recording before hand, as the MICE dataset contains images of 640x640 pixels of resolution.\n",
    "\n",
    "For this reason, we need to format the video so its resolution matches those used for the PrecisionTrack and DLC evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling video from 1536x1536 to 640x640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Video(backend=MediaVideo(filename='../../assets/20mice_640x640.avi', grayscale=False, bgr=True, dataset='', input_format='channels_last'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_video = sleap.load_video(cfg.video_path)\n",
    "ori_width = ori_video.backend.width\n",
    "ori_height = ori_video.backend.height\n",
    "\n",
    "if ori_height != cfg.img_size[0] or ori_width != cfg.img_size[1]:\n",
    "    print(\n",
    "        f\"Rescaling video from {ori_height}x{ori_width} to {cfg.img_size[0]}x{cfg.img_size[1]}\"\n",
    "    )\n",
    "    split = osp.splitext(cfg.video_path)\n",
    "    rescaled_video_path = f\"{split[0]}_{cfg.img_size[0]}x{cfg.img_size[1]}{split[1]}\"\n",
    "    video_path = downsize_video(\n",
    "        cfg.video_path,\n",
    "        rescaled_video_path,\n",
    "        height=cfg.img_size[0],\n",
    "        width=cfg.img_size[1],\n",
    "    )\n",
    "    video = sleap.load_video(rescaled_video_path)\n",
    "    scale = (ori_height / cfg.img_size[0], ori_width / cfg.img_size[1])\n",
    "else:\n",
    "    video = ori_video\n",
    "    scale = (1, 1)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Benchmarking\n",
    "\n",
    "SLEAP splits its inference process into two parts:\n",
    "1. Running the neural network model to obtain the poses on all the frames in the video.\n",
    "2. Tracking instances over the video by assigning poses over all the frames iteratively.\n",
    "\n",
    "Here, we provide the latency for both steps and the cumulative latency. This last one will be the latency you will actually experience in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/mambaforge/envs/sleap/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e953cdb5c6b4488b9c7e4d07ec1d0043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| SLEAP: Prediction step   |   Inference Time (s) |\n",
      "|--------------------------|----------------------|\n",
      "| Mean                     |                0.085 |\n",
      "| Std                      |                0.442 |\n",
      "| Min                      |                0.014 |\n",
      "| Median                   |                0.018 |\n",
      "| Max                      |                3.016 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1471it [00:41, 35.72it/s]\n",
      "1471it [00:06, 210.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total mean delay is 0.10159 seconds which gives us a total latency of 9.84 FPS\n"
     ]
    }
   ],
   "source": [
    "tracker = sleap.nn.tracking.Tracker.make_tracker_by_name(\n",
    "    tracker=\"simplemaxtracks\",\n",
    "    track_window=5,\n",
    "    similarity=\"iou\",\n",
    "    match=\"hungarian\",\n",
    "    min_new_track_points=1,\n",
    "    min_match_points=1,\n",
    "    target_instance_count=cfg.max_tracks,\n",
    "    pre_cull_to_target=True,\n",
    "    pre_cull_iou_threshold=0.8,\n",
    "    post_connect_single_breaks=True,\n",
    "    clean_instance_count=0,\n",
    "    clean_iou_threshold=None,\n",
    "    max_tracking=True,\n",
    "    max_tracks=cfg.max_tracks,\n",
    "    candidate_maker=\"SimpleMaxTracksCandidateMaker\",\n",
    ")\n",
    "predictor = load_model(cfg.load_from, batch_size=30)\n",
    "\n",
    "\n",
    "predictions, predictions_delay = predictor.predict(video)\n",
    "\n",
    "mean_pred_delay = display_latency(predictions_delay, \"SLEAP: Prediction step\")\n",
    "results = SLEAPOutput(save_path_bboxes=cfg.save_path_bboxes, save_path_kpts=cfg.save_path_kpts, precision=32)\n",
    "\n",
    "lfs = []\n",
    "times = []\n",
    "for i, lf in tqdm(enumerate(predictions)):\n",
    "    t0 = perf_counter()\n",
    "    lf.instances = tracker.track(lf.instances, img=lf.image, t=i, img_hw=lf.image.shape[:2])\n",
    "    lfs.append(lf)\n",
    "    times.append(perf_counter() - t0)\n",
    "\n",
    "times = np.array(times) / 2  # Simulates two threads perfectly running concurrently\n",
    "mean_track_delay = np.mean(times)\n",
    "\n",
    "t1 = perf_counter()\n",
    "tracker.final_pass(lfs)\n",
    "mean_track_delay += (perf_counter()-t1)\n",
    "\n",
    "for i, lf in tqdm(enumerate(lfs)):\n",
    "    results(lf, *scale)\n",
    "results.save()\n",
    "\n",
    "print(f\"The total mean delay is {mean_pred_delay + mean_track_delay:.5f} seconds which gives us a total latency of {1/(mean_pred_delay+mean_track_delay):.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation\n",
    "\n",
    "Here are the metrics regarding the quality of the tracking relative to the ground truth. Check the paper for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:partials: 0.054 seconds.\n",
      "INFO:root:mergeOverall: 0.055 seconds.\n",
      "\n",
      "| Metric                  |     Score |\n",
      "|-------------------------|-----------|\n",
      "| MOTA on mouse           |     0.725 |\n",
      "| IDF1 on mouse           |     0.717 |\n",
      "| IDP on mouse            |     0.722 |\n",
      "| IDR on mouse            |     0.711 |\n",
      "| Precision on mouse      |     0.868 |\n",
      "| Recall on mouse         |     0.856 |\n",
      "| IDFP on mouse           |  8008.000 |\n",
      "| IDFN on mouse           |  8436.000 |\n",
      "| IDTP on mouse           | 20780.000 |\n",
      "| Num Switches on mouse   |    23.000 |\n",
      "| Num Detections on mouse | 25000.000 |\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(cfg.save_path_bboxes)\n",
    "results.drop(columns=[\"class_id\"], inplace=True)\n",
    "results = results.values\n",
    "gt = pd.read_csv(cfg.gt_path).values\n",
    "evaluator = MOTEvaluation(classes=[\"mouse\"])\n",
    "unique_frames = np.unique(gt[:, 0])\n",
    "\n",
    "for frame in unique_frames:\n",
    "    frame_gt = gt[gt[:, 0] == frame]\n",
    "    frame_gt = {\"mouse\": frame_gt[:, 2:-1]}\n",
    "    frame_results = results[results[:, 0] == frame]\n",
    "    frame_results = {\"mouse\": frame_results[:, 1:-1]}\n",
    "    evaluator.update(frame_results, frame_gt)\n",
    "\n",
    "ev = evaluator.evaluate()\n",
    "display_mot_results(ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation of the tracking results\n",
    "\n",
    "Youre PrecisionTrack formatted outputs are available at cfg.save_dir_mot. You can leverage PrecisionTrack's visualization tool to proceed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
